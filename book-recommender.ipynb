{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "PO4XCHGftllK"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'numpy'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpairwise\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cosine_similarity\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'numpy'"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pickle\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fibuCSQtqH8"
      },
      "source": [
        "# 1. Load and Preprocess Data\n",
        "# In this section, we load the raw datasets, perform initial cleaning by renaming columns for consistency, and merge them into a single comprehensive DataFrame."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-j1jUClt7bX"
      },
      "source": [
        "Data Loading and Initial Merging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ITqREOMNtliz",
        "outputId": "eee6813f-2f80-4c2d-d0eb-5ae0407f872c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading datasets...\n",
            "Cleaning and preprocessing data...\n",
            "Data loaded and preprocessed.\n",
            "Ratings with book names shape: (254997, 10)\n"
          ]
        }
      ],
      "source": [
        "print(\"Loading datasets...\")\n",
        "# --- FIX: Added encoding='latin-1' to handle special characters in the data ---\n",
        "books = pd.read_csv('Books.csv', low_memory=False, encoding='latin-1', on_bad_lines='skip')\n",
        "ratings = pd.read_csv('Ratings.csv', low_memory=False, encoding='latin-1', on_bad_lines='skip')\n",
        "\n",
        "print(\"Cleaning and preprocessing data...\")\n",
        "books.rename(columns={'Book-Title':'title', 'Book-Author':'author', 'Year-Of-Publication':'year', 'Publisher':'publisher', 'Image-URL-L':'image_url'}, inplace=True)\n",
        "ratings.rename(columns={'User-ID':'user_id', 'Book-Rating':'rating'}, inplace=True)\n",
        "\n",
        "# Merge ratings and books on the 'ISBN' column\n",
        "ratings_with_name = ratings.merge(books, on='ISBN')\n",
        "\n",
        "print(\"Data loaded and preprocessed.\")\n",
        "print(\"Ratings with book names shape:\", ratings_with_name.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6GLZXicPuI4C"
      },
      "source": [
        "# 2. Build Popularity-Based Recommender Data\n",
        "# This model recommends books based on simple popularity. We identify the most popular books by considering only those with a significant number of ratings (>= 250) and then sorting them by their average rating."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mhXBzhgujvO"
      },
      "source": [
        " Calculate Popularity Metrics and Create popular_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DtrKp3MZtlgR",
        "outputId": "86cc3969-148f-4f86-c5c5-8e68f7defb3a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Building popularity model data...\n",
            "Popularity DataFrame created. Shape: (30, 5)\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nBuilding popularity model data...\")\n",
        "# Calculate the number of ratings for each book\n",
        "num_rating_df = ratings_with_name.groupby('title').count()['rating'].reset_index()\n",
        "num_rating_df.rename(columns={'rating':'num_ratings'}, inplace=True)\n",
        "\n",
        "# Calculate the average rating for each book\n",
        "avg_rating_df = ratings_with_name.groupby('title').mean(numeric_only=True)['rating'].reset_index()\n",
        "avg_rating_df.rename(columns={'rating':'avg_rating'}, inplace=True)\n",
        "\n",
        "# Merge to create a dataframe with both metrics\n",
        "popular_df = num_rating_df.merge(avg_rating_df, on='title')\n",
        "\n",
        "# Filter for books with at least 250 ratings\n",
        "popular_df = popular_df[popular_df['num_ratings'] >= 250].sort_values('avg_rating', ascending=False)\n",
        "\n",
        "# Merge with the main books dataframe to get details (author, image_url)\n",
        "popular_df = popular_df.merge(books, on='title').drop_duplicates('title')[['title', 'author', 'image_url', 'num_ratings', 'avg_rating']]\n",
        "\n",
        "print(\"Popularity DataFrame created. Shape:\", popular_df.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWaXrNCruvPE"
      },
      "source": [
        "# 3. Build Collaborative Filtering Model\n",
        "# This model is based on the \"wisdom of the crowd.\" It recommends books by\n",
        "# finding patterns in user ratings. We filter the data to include only\n",
        "# experienced users and frequently-rated books to reduce noise."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_f1cCwKWu47z"
      },
      "source": [
        "Filter Data and Build User-Item Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bIoT21KRtleL",
        "outputId": "39fbb081-7302-4d86-9a24-53682c9dffb4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Building collaborative filtering model...\n",
            "Pivot table for collaborative filtering created. Shape: (56, 177)\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nBuilding collaborative filtering model...\")\n",
        "# Filter for users who have rated more than 200 books\n",
        "x = ratings_with_name.groupby('user_id').count()['rating'] > 200\n",
        "exp_users = x[x].index\n",
        "filtered_rating = ratings_with_name[ratings_with_name['user_id'].isin(exp_users)]\n",
        "\n",
        "# Filter for books with at least 50 ratings from this experienced group\n",
        "y = filtered_rating.groupby('title').count()['rating'] >= 50\n",
        "famous_books = y[y].index\n",
        "final_ratings = filtered_rating[filtered_rating['title'].isin(famous_books)]\n",
        "\n",
        "# Create the user-item pivot table\n",
        "pt = final_ratings.pivot_table(index='title', columns='user_id', values='rating')\n",
        "pt.fillna(0, inplace=True)\n",
        "\n",
        "print(\"Pivot table for collaborative filtering created. Shape:\", pt.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5X_SGYipu-RX"
      },
      "source": [
        "Calculate Collaborative Similarity\n",
        "\n",
        "We use Cosine Similarity to measure how similarly books were rated by users."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6n5hWibxtlbo",
        "outputId": "fc8e3983-60ae-4ec1-a59c-28a26318b212"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collaborative similarity matrix created. Shape: (56, 56)\n"
          ]
        }
      ],
      "source": [
        "collaborative_similarity_scores = cosine_similarity(pt)\n",
        "print(\"Collaborative similarity matrix created. Shape:\", collaborative_similarity_scores.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jjfrMQjHvI40"
      },
      "source": [
        " # 4. Build Content-Based Filtering Model\n",
        "# This model recommends books based on their content attributes (author and\n",
        "# publisher). It uses TF-IDF to convert text features into numerical vectors,\n",
        "# which can then be compared for similarity."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNeAwbMVvOOj"
      },
      "source": [
        " Feature Engineering and Content Similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F-2x0ObgtlS2",
        "outputId": "deee07c8-16ec-4e39-81f4-bb5958c993a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Building content-based filtering model...\n",
            "Content-based similarity matrix created. Shape: (56, 56)\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nBuilding content-based filtering model...\")\n",
        "# Use books that are present in the collaborative model for consistency\n",
        "content_df = books[books['title'].isin(pt.index)].copy()\n",
        "content_df.drop_duplicates(subset='title', inplace=True)\n",
        "content_df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# Create 'tags' from author and publisher\n",
        "content_df['tags'] = content_df['author'] + \" \" + content_df['publisher']\n",
        "content_df['tags'] = content_df['tags'].fillna('').astype(str)\n",
        "\n",
        "# Vectorize tags using TF-IDF\n",
        "tfidf = TfidfVectorizer(stop_words='english')\n",
        "tfidf_matrix = tfidf.fit_transform(content_df['tags'])\n",
        "\n",
        "# Calculate similarity\n",
        "content_similarity_scores = cosine_similarity(tfidf_matrix)\n",
        "print(\"Content-based similarity matrix created. Shape:\", content_similarity_scores.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHvHCz-gvVCb"
      },
      "source": [
        "# 5. Offline Model Evaluation\n",
        "\n",
        "# To validate our collaborative model, we perform an offline evaluation. We split the data into a training set (past) and a testing set (future) and measure how well the model can predict books a user will like."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aUvVWJFHvZ7W"
      },
      "source": [
        "Run Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UY2nKSONvSsh",
        "outputId": "20ac3fac-e243-40fd-d0f1-d4b50538e3ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Starting Offline Model Evaluation ---\n",
            "Evaluation Results (k=5):\n",
            "Average Precision@5: 0.2444\n",
            "Average Recall@5: 0.3337\n",
            "---------------------------------------\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n--- Starting Offline Model Evaluation ---\")\n",
        "train_data, test_data = train_test_split(final_ratings, test_size=0.2, random_state=42)\n",
        "train_pt = train_data.pivot_table(index='title', columns='user_id', values='rating')\n",
        "train_pt.fillna(0, inplace=True)\n",
        "train_similarity_scores = cosine_similarity(train_pt)\n",
        "\n",
        "def calculate_precision_recall(test_data, train_pt, similarity_scores, k=5):\n",
        "    title_to_index = {title: i for i, title in enumerate(train_pt.index)}\n",
        "    test_users = test_data['user_id'].unique()\n",
        "    total_precision, total_recall, processed_users = 0, 0, 0\n",
        "\n",
        "    for user_id in test_users:\n",
        "        true_positives = set(test_data[test_data['user_id'] == user_id]['title'])\n",
        "        train_positives = set(train_data[train_data['user_id'] == user_id]['title'])\n",
        "        if not train_positives or not true_positives: continue\n",
        "\n",
        "        last_liked_book = list(train_positives)[-1]\n",
        "        if last_liked_book not in title_to_index: continue\n",
        "\n",
        "        index = title_to_index[last_liked_book]\n",
        "        similar_items = sorted(list(enumerate(similarity_scores[index])), key=lambda x: x[1], reverse=True)[1:k+1]\n",
        "        recommended_titles = {train_pt.index[i[0]] for i in similar_items}\n",
        "\n",
        "        hits = len(recommended_titles.intersection(true_positives))\n",
        "        if hits > 0:\n",
        "            precision = hits / k\n",
        "            recall = hits / len(true_positives)\n",
        "            total_precision += precision\n",
        "            total_recall += recall\n",
        "            processed_users += 1\n",
        "\n",
        "    avg_precision = total_precision / processed_users if processed_users > 0 else 0\n",
        "    avg_recall = total_recall / processed_users if processed_users > 0 else 0\n",
        "    return avg_precision, avg_recall\n",
        "\n",
        "precision_at_5, recall_at_5 = calculate_precision_recall(test_data, train_pt, train_similarity_scores, k=5)\n",
        "print(f\"Evaluation Results (k=5):\")\n",
        "print(f\"Average Precision@5: {precision_at_5:.4f}\")\n",
        "print(f\"Average Recall@5: {recall_at_5:.4f}\")\n",
        "print(\"---------------------------------------\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqn2HY_bvgOO"
      },
      "source": [
        "# 6. Export Final Artifacts\n",
        "\n",
        "# Finally, we save all the necessary Python objects (DataFrames and similarity matrices) into a single .pkl file. The Flask application will load this file to make live recommendations without re-running the analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JnRL_0qnvlL7"
      },
      "source": [
        "Save the .pkl file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KpJ5j73yvSp8",
        "outputId": "7807c96a-1204-41c5-f873-39ce5b117a6c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Exporting all production data to recommender_data.pkl...\n",
            "\n",
            "-------------------------------------------------\n",
            "All models built and data exported successfully!\n",
            "-------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nExporting all production data to recommender_data.pkl...\")\n",
        "recommender_data = {\n",
        "    'popular_df': popular_df,\n",
        "    'pt': pt,\n",
        "    'books': books,\n",
        "    'collaborative_similarity_scores': collaborative_similarity_scores,\n",
        "    'content_df': content_df,\n",
        "    'content_similarity_scores': content_similarity_scores\n",
        "}\n",
        "pickle.dump(recommender_data, open('recommender_data.pkl', 'wb'))\n",
        "\n",
        "print(\"\\n-------------------------------------------------\")\n",
        "print(\"All models built and data exported successfully!\")\n",
        "print(\"-------------------------------------------------\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
